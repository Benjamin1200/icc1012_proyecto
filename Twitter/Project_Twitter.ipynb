{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter\n",
    "\n",
    "In this notebook we are going to show all the used for the analysis. Also we are going to show all the graphics asociated with the data obtaines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We downloaded data from twitter with various methods, which did not provide the same amount of data, so we standarized to the minimal amount which it could be useful. Those fields are: \n",
    "* Date when it was created at.\n",
    "* ID of the tweet, which is unique.\n",
    "* The text of the tweet.\n",
    "* The user, which contains the user screen name (username) and the user ID.\n",
    "\n",
    "After defining that the period of downloading data was finished, the amount of data gathered was: **6.542.718** unique tweets. Which is a decent amount of data to analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "1) The first step is to filter the data in different files that will be used for all the analysis. The filter will be keywords (in the meantime), the keywords are the platforms name (i.e. Nintendo, Playstation, Xbox), these keywords are searched in the text and the username. If there is no defined preference (not zero preference) in which platform the record was pointing to, it is inserted in both files.\n",
    "\n",
    "The ideal method to filter the data would be create a database of keywords asociated with each platform, so in every record when searching it could be calculated the probability of that text (according to all the words) to which platform it goes. This would require a model with N-Bayes, but like most predictive models it requires training, which we do not have at this moment.\n",
    "\n",
    "**Note**: Also from here on we called each file as follows:\n",
    "* project_tweets01.data -> Nintendo\n",
    "* project_tweets02.data -> Playstation\n",
    "* project_tweets03.data -> Xbox\n",
    "* project_tweets04.data -> Else (Which is everything else that did not fit in the other categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    nintendo_platform_file = []\n",
    "    playstation_platform_file = []\n",
    "    xbox_platform_file = []\n",
    "    else_platform_file = []\n",
    "    _file = open(\"project_tweets.data\", 'r')\n",
    "    for line in _file.readlines():\n",
    "        data = json.loads(line)\n",
    "        try:\n",
    "            raw_text = data['text'].strip().upper()\n",
    "            raw_username = data['user']['screen_name'].strip().upper()\n",
    "        except:\n",
    "            # No idea why would happen.\n",
    "            print \"WTF!\"\n",
    "            raw_text = \"\"\n",
    "            raw_username = \"\"\n",
    "        nintendo_mentions = raw_text.count(\"NINTENDO\")\n",
    "        playstation_mentions = raw_text.count(\"PLAYSTATION\")\n",
    "        xbox_mentions = raw_text.count(\"XBOX\")\n",
    "        nintendo_mentions += 1 if raw_username.find(\"NINTENDO\") != -1 else 0\n",
    "        playstation_mentions += 1 if raw_username.find(\"PLAYSTATION\") != -1 else 0\n",
    "        xbox_mentions += 1 if raw_username.find(\"XBOX\") != -1 else 0\n",
    "        if nintendo_mentions > playstation_mentions and nintendo_mentions > xbox_mentions:\n",
    "            nintendo_platform_file.append(line)\n",
    "        elif playstation_mentions > nintendo_mentions and playstation_mentions > xbox_mentions:\n",
    "            playstation_platform_file.append(line)\n",
    "        elif xbox_mentions > playstation_mentions and xbox_mentions > nintendo_mentions:\n",
    "            xbox_platform_file.append(line)\n",
    "        elif nintendo_mentions == playstation_mentions and nintendo_mentions == xbox_mentions and nintendo_mentions > 0:\n",
    "            nintendo_platform_file.append(line)\n",
    "            playstation_platform_file.append(line)\n",
    "            xbox_platform_file.append(line)\n",
    "        elif nintendo_mentions == playstation_mentions and nintendo_mentions != xbox_mentions:\n",
    "            nintendo_platform_file.append(line)\n",
    "            playstation_platform_file.append(line)\n",
    "        elif nintendo_mentions == xbox_mentions and nintendo_mentions != playstation_mentions:\n",
    "            nintendo_platform_file.append(line)\n",
    "            xbox_platform_file.append(line)\n",
    "        elif playstation_mentions == xbox_mentions and playstation_mentions != nintendo_mentions:\n",
    "            playstation_platform_file.append(line)\n",
    "            xbox_platform_file.append(line)\n",
    "        else:\n",
    "            else_platform_file.append(line)\n",
    "    filenames = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for filename in filenames:\n",
    "        # Clean files.\n",
    "        open(filename, 'w').close()\n",
    "        # Write files in append mode.\n",
    "        with open(filename, 'a') as _file:\n",
    "            if filename == \"project_tweets01.data\":\n",
    "                for line in nintendo_platform_file:\n",
    "                    _file.write(line)\n",
    "            elif filename == \"project_tweets02.data\":\n",
    "                for line in playstation_platform_file:\n",
    "                    _file.write(line)\n",
    "            elif filename == \"project_tweets03.data\":\n",
    "                for line in xbox_platform_file:\n",
    "                    _file.write(line)\n",
    "            else:\n",
    "                for line in else_platform_file:\n",
    "                    _file.write(line)\n",
    "    time_end = time.clock()\n",
    "\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The first analysis to do with this data that is now filtered by platforms, is to count the amount of records per platform. This is done with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    # Clean File.\n",
    "    open(\"records_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\", \"project_tweets.data\"]\n",
    "    for _file in files:\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"records_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        _data = open(_file, 'r')\n",
    "        lines = _data.readlines()\n",
    "        records = len(lines)\n",
    "        print records, '\\n'\n",
    "\n",
    "    time_end = time.clock()\n",
    "\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "project_tweets01.data\n",
    "\n",
    "1317667 \n",
    "\n",
    "project_tweets02.data\n",
    "\n",
    "2337881 \n",
    "\n",
    "project_tweets03.data\n",
    "\n",
    "2260157 \n",
    "\n",
    "project_tweets04.data\n",
    "\n",
    "735410 \n",
    "\n",
    "project_tweets.data\n",
    "\n",
    "6542718 \n",
    "\n",
    "Time taken to completion of the metric: 37.615491 in processor time\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see that in the \"else\" category we have **11.24%** of all the data, which is not a small amount. But considering that our filter for the platforms is kind of brute force is all right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The next analysis to do is to count the amount of unique users per platform. This is done with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    # Clean File.\n",
    "    open(\"user_amount_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for _file in files:\n",
    "        # parameters for mrjob.\n",
    "        # To run your job in multiple subprocesses with a few Hadoop features simulated, use -r local.\n",
    "        option1 = \"\" #\"\"-r\"\n",
    "        option2 = \"\" #\"\"local\"\n",
    "        sys.argv = ['user_amount.py', option1, option2, _file]\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"user_amount_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        execfile('user_amount.py')\n",
    "        print \"\\n\"\n",
    "\n",
    "    time_end = time.clock()\n",
    "\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "import time\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, record):\n",
    "        yield [record['user']['screen_name'], 1]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield [key, 1]\n",
    "\n",
    "    def mapper2(self, key, values):\n",
    "        yield ['amount_users', values]\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        yield [key, sum(values)]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer),\n",
    "                MRStep(mapper=self.mapper2, reducer=self.reducer2)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #time_start = time.clock()\n",
    "    MRWordFrequencyCount().run()\n",
    "    #time_end = time.clock()\n",
    "    #print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "project_tweets01.data\n",
    "\n",
    "\"amount_users\"\t426450\n",
    "\n",
    "\n",
    "project_tweets02.data\n",
    "\n",
    "\"amount_users\"\t574308\n",
    "\n",
    "\n",
    "project_tweets03.data\n",
    "\n",
    "\"amount_users\"\t727273\n",
    "\n",
    "\n",
    "project_tweets04.data\n",
    "\n",
    "\"amount_users\"\t265827\n",
    "\n",
    "\n",
    "Time taken to completion of the metric: 426.030981 in processor time\n",
    "\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see that in the Xbox platform there are more unique users than in all the other platforms by a not small percentage, it almost duplicates Nintendo unique users. What is interesting, is using the analysis from before we can see that Xbox had less records than Playstation, but seeing this there is a whooping 150k (estimated) more unique users in Xbox, which could lead us that Playstation content creation is more for each unique user or that there is a tiny amount of users that produce all the content for this platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) The next analysis is to calculate the Top 10 users that generate the most content in each platform. This is done with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    # Clean File.\n",
    "    open(\"top_users_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for _file in files:\n",
    "        # parameters for mrjob.\n",
    "        # To run your job in multiple subprocesses with a few Hadoop features simulated, use -r local.\n",
    "        option1 = \"\" #\"\"-r\"\n",
    "        option2 = \"\" #\"\"local\"\n",
    "        sys.argv = ['top_users.py', option1, option2, _file]\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"top_users_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        execfile('top_users.py')\n",
    "        print \"\\n\"\n",
    "\n",
    "    time_end = time.clock()\n",
    "\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "import time\n",
    "import itertools\n",
    "import operator\n",
    "import sys\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, record):\n",
    "        yield [record['user']['screen_name'], 1]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield [\"top_user\", (sum(values), key)]\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        user_ids = []\n",
    "        user_tweets = []\n",
    "        for value in values:\n",
    "            user_ids.append(value[1])\n",
    "            user_tweets.append(value[0])\n",
    "        user = {}\n",
    "        for i in xrange(0, len(user_ids)):\n",
    "            user[user_ids[i]] = user_tweets[i]\n",
    "        top_users = sorted(user.items(), key=lambda x: (x[1], operator.itemgetter(0)), reverse=True)\n",
    "        for user in top_users[0:10]:\n",
    "            #print user[0], user[1]\n",
    "            yield [user[0], user[1]]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer),\n",
    "                MRStep(reducer=self.reducer2)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #time_start = time.clock()\n",
    "    MRWordFrequencyCount().run()\n",
    "    #time_end = time.clock()\n",
    "    #print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "project_tweets01.data\n",
    "* \"savetimeandmoey\"\t11391\n",
    "* \"AuctionPorn\"\t8449\n",
    "* \"AmazonBay4u\"\t8352\n",
    "* \"retrodeals\"\t7435\n",
    "* \"Nintendo_Legend\"\t6895\n",
    "* \"retrodealsUK\"\t6744\n",
    "* \"LastChanceGamer\"\t5904\n",
    "* \"GameUP247\"\t5580\n",
    "* \"RetroNuss\"\t5441\n",
    "* \"Nintendoe3E3\"\t5438\n",
    "\n",
    "\n",
    "project_tweets02.data\n",
    "* \"Cammie_Whybrew\"\t13616\n",
    "* \"AskPlayStation\"\t13282\n",
    "* \"eBayShopperNews\"\t11578\n",
    "* \"VideoGamesMall\"\t10274\n",
    "* \"savetimeandmoey\"\t8632\n",
    "* \"collinschristof\"\t5382\n",
    "* \"topnewskoeln\"\t5297\n",
    "* \"Gamifive\"\t4829\n",
    "* \"Xbox_360_Gamez\"\t4080\n",
    "* \"pressebank\"\t3799\n",
    "\n",
    "\n",
    "project_tweets03.data\n",
    "* \"Xbox_360_Gamez\"\t34137\n",
    "* \"XboxSupport\"\t14421\n",
    "* \"VideoGamesMall\"\t11835\n",
    "* \"Xbox_One_Reddit\"\t10215\n",
    "* \"xboxgamersdeals\"\t9224\n",
    "* \"GameUP247\"\t9156\n",
    "* \"KingsleyNewz\"\t9139\n",
    "* \"bullzyy\"\t7576\n",
    "* \"savetimeandmoey\"\t7176\n",
    "* \"giveawayxfab\"\t7028\n",
    "\n",
    "\n",
    "project_tweets04.data\n",
    "* \"savetimeandmoey\"\t15483\n",
    "* \"VideoGames_Up\"\t13804\n",
    "* \"giveawaygigatop\"\t8878\n",
    "* \"tw100_1\"\t6538\n",
    "* \"videogames_pt\"\t6357\n",
    "* \"videogames_fr\"\t5993\n",
    "* \"giveawayxfab\"\t5886\n",
    "* \"DMGG_Videogames\"\t5295\n",
    "* \"ShoppeWorld\"\t4824\n",
    "* \"VideoGames_TV\"\t3709\n",
    "\n",
    "\n",
    "Time taken to completion of the metric: 335.930534 in processor time\n",
    "\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see that there are some users in more than one platform, and that they are one of the top 10 users that generate contents, which means that they are probably spammers, bots or someone really good at making records for almost all the platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) The nexts analysis of the data is by each hour of a day in every week (the data was gathered).\n",
    "\n",
    "5.1) We now calculate the Top 20 words. That is how many times a single word was used in each record for each platform. This is done with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import operator\n",
    "\n",
    "# The summary is for each hour per week. (i.e. 20 times hello on week 1, 31 times bye on week 2, ...)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    # Clean File.\n",
    "    open(\"count_words_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for _file in files:\n",
    "        punctuation = []\n",
    "        open(_file[:-5] + \"_words.data\", 'w').close()\n",
    "        _output = open(_file[:-5] + \"_words.data\", 'a')\n",
    "        _output_filename = _output.name\n",
    "        for i in xrange(0, len(string.punctuation)):\n",
    "            punctuation.append(string.punctuation[i])\n",
    "        stop_words = stopwords.words() + punctuation\n",
    "        _data = open(_file, 'r')\n",
    "        for line in _data.readlines():\n",
    "            data = json.loads(line)\n",
    "            date = parse(data['created_at'])\n",
    "            week = date.isocalendar()[1]\n",
    "            hour = date.hour\n",
    "            _words = [i for i in word_tokenize(data['text'].lower()) if i not in stop_words]\n",
    "            for word in _words:\n",
    "                record = {'word': word, 'week': week, 'hour': hour}\n",
    "                _output.write(json.dumps(record, sort_keys=True) + \"\\n\")\n",
    "        _data.close()\n",
    "        _output.close()\n",
    "        # parameters for mrjob.\n",
    "        # To run your job in multiple subprocesses with a few Hadoop features simulated, use -r local.\n",
    "        option1 = \"\" #\"\"-r\"\n",
    "        option2 = \"\" #\"\"local\"\n",
    "        sys.argv = ['words_amount.py', option1, option2, _output_filename]\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"count_words_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        execfile('words_amount.py')\n",
    "        print \"\\n\"\n",
    "\n",
    "    time_end = time.clock()\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "import time\n",
    "import itertools\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import operator\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, record):\n",
    "        hour = record['hour']\n",
    "        week = record['week']\n",
    "        word = record['word']\n",
    "        yield [(week, hour, word), 1]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield [(key[0], key[1]), (sum(values), key[2])]\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        counts = []\n",
    "        words = []\n",
    "        for value in values:\n",
    "            words.append(value[1])\n",
    "            counts.append(value[0])\n",
    "        pair_word_count = {}\n",
    "        for i in xrange(0, len(words)):\n",
    "            pair_word_count[words[i]] = counts[i]\n",
    "        top_words = sorted(pair_word_count.items(), key=lambda x: (x[1], operator.itemgetter(0)), reverse=True)\n",
    "        for word in top_words[0:20]:\n",
    "            #print user[0], user[1]\n",
    "            yield [(key[0], key[1], word[0]), word[1]]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer), MRStep(reducer=self.reducer2)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #time_start = time.clock()\n",
    "    MRWordFrequencyCount().run()\n",
    "    #time_end = time.clock()\n",
    "    #print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see which hours in which week. The Top 20 most used words in all of that period records by platform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Graphic **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2) We now calculate how many records have a sentiment of \"positiveness\", \"negativeness\" or \"neutralness\" for each platform with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import sentiment_analyzer as sa\n",
    "\n",
    "# The summary is for each hour per week. (i.e. 20 positives tweets on week 1, 31 negative tweets on week 2, ...)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    sid = sa.init_classifier()\n",
    "    # Clean File.\n",
    "    open(\"sentiment_analysis_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for _file in files:\n",
    "        _output_filename = sa.classify_file(_file, sid)\n",
    "        # parameters for mrjob.\n",
    "        # To run your job in multiple subprocesses with a few Hadoop features simulated, use -r local.\n",
    "        option1 = \"\" #\"\"-r\"\n",
    "        option2 = \"\" #\"\"local\"\n",
    "        sys.argv = ['sentiment_amount.py', option1, option2, _output_filename]\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"sentiment_analysis_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        execfile('sentiment_amount.py')\n",
    "        print \"\\n\"\n",
    "\n",
    "    time_end = time.clock()\n",
    "\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def init_classifier():\n",
    "    n_instances = 100\n",
    "    subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "    obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "\n",
    "    train_subj_docs = subj_docs[:80]\n",
    "    test_subj_docs = subj_docs[80:100]\n",
    "    train_obj_docs = obj_docs[:80]\n",
    "    test_obj_docs = obj_docs[80:100]\n",
    "    training_docs = train_subj_docs + train_obj_docs\n",
    "    testing_docs = test_subj_docs + test_obj_docs\n",
    "\n",
    "    sentim_analyzer = SentimentAnalyzer()\n",
    "    all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "    unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "    training_set = sentim_analyzer.apply_features(training_docs)\n",
    "    test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "    trainer = NaiveBayesClassifier.train\n",
    "    classifier = sentim_analyzer.train(trainer, training_set)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid\n",
    "\n",
    "\n",
    "def classify_file(_file, sid):\n",
    "    _data = open(_file, 'r')\n",
    "    open(_file[:-5] + \"_classified.data\", 'w').close()\n",
    "    _output = open(_file[:-5] + \"_classified.data\", 'a')\n",
    "    for line in _data.readlines():\n",
    "        record = json.loads(line)\n",
    "        ss = sid.polarity_scores(record['text'].encode('utf-8'))\n",
    "        if ss['compound'] < 0:\n",
    "            classification = \"neg\"\n",
    "        elif ss['compound'] == 0:\n",
    "            classification = \"neu\"\n",
    "        else:\n",
    "            classification = \"pos\"\n",
    "        try:\n",
    "            record.update({'classification': classification})\n",
    "            _output.write(json.dumps(record, sort_keys=True) + \"\\n\")\n",
    "        except:\n",
    "            # Should not happen.\n",
    "            print \"WTF!\"\n",
    "    _data.close()\n",
    "    _output.close()\n",
    "    return _output.name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "import time\n",
    "import itertools\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, record):\n",
    "        date = parse(record['created_at'])\n",
    "        week = date.isocalendar()[1]\n",
    "        hour = date.hour\n",
    "        yield [(week, hour, record['classification']), 1]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield [key, sum(values)]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #time_start = time.clock()\n",
    "    MRWordFrequencyCount().run()\n",
    "    #time_end = time.clock()\n",
    "    #print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see which hours in which week had more positive or negative records by platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graphic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "archivo = open(\"sentiment_analysis_by_platform_summary.txt\")\n",
    "archivo.readline()\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "chars_to_remove = [',', '[', ']', '\"']\n",
    "for i in archivo:\n",
    "\tline = i.translate(None, ''.join(chars_to_remove))\n",
    "\tline = line.split()\n",
    "\tif line[2] == \"pos\":\n",
    "\t\tpos.append([int(line[0]), int(line[1]), int(line[3])])\n",
    "\telif line[2] == \"neg\":\n",
    "\t\tneg.append([int(line[0]), int(line[1]), int(line[3])])\n",
    "\telse:\n",
    "\t\tneu.append([int(line[0]), int(line[1]), int(line[3])])\n",
    "\n",
    "x_neg = []\n",
    "x_neg_name = []\n",
    "y_neg = []\n",
    "count = 0\n",
    "for i in sorted(neg):\n",
    "\tx_neg.append(count)\n",
    "\tx_neg_name.append(str(i[0])+ \" \" + str(i[1]))\n",
    "\ty_neg.append(i[2])\n",
    "\tcount += 1\n",
    "\n",
    "plt.xticks(x_neg, x_neg_name)\n",
    "neg_plt = plt.plot(x_neg, y_neg, 'r',label='negatives')\n",
    "\n",
    "x_neu = []\n",
    "x_neu_name = []\n",
    "y_neu = []\n",
    "count = 0\n",
    "for i in sorted(neu):\n",
    "\tx_neu.append(count)\n",
    "\tx_neu_name.append(str(i[0])+ \" \" + str(i[1]))\n",
    "\ty_neu.append(i[2])\n",
    "\tcount += 1\n",
    "\n",
    "plt.xticks(x_neu, x_neu_name)\n",
    "neu_plt = plt.plot(x_neu, y_neu, 'b',label='neutral')\n",
    "\n",
    "\n",
    "x_pos = []\n",
    "x_pos_name = []\n",
    "y_pos = []\n",
    "count = 0\n",
    "for i in sorted(pos):\n",
    "\tx_pos.append(count)\n",
    "\tx_pos_name.append(str(i[0])+ \" \" + str(i[1]))\n",
    "\ty_pos.append(i[2])\n",
    "\tcount += 1\n",
    "\n",
    "plt.xticks(x_pos, x_pos_name)\n",
    "pos_plt = plt.plot(x_pos, y_pos, 'g', label='positive')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3) We now calculate how many records are from the different languages for each platform with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import language_detect as ld\n",
    "\n",
    "# The summary is for each hour per week. (i.e. 20 spanish tweets on week 1, 31 english tweets on week 2, ...)\n",
    "\n",
    "# Important Note: This is just an estimation of the language based on the stopwords, so if no stopwords are found the language\n",
    "#                   that will produce is garbage, but is not the fault of the model. Also if there are some stopwords of both\n",
    "#                   languages it will say is the one with most stopwords.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    # Clean File.\n",
    "    open(\"languages_amount_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for _file in files:\n",
    "        _output_filename = ld.set_language_for_each_tweet(_file)\n",
    "        # parameters for mrjob.\n",
    "        # To run your job in multiple subprocesses with a few Hadoop features simulated, use -r local.\n",
    "        option1 = \"\" #\"\"-r\"\n",
    "        option2 = \"\" #\"\"local\"\n",
    "        sys.argv = ['count_language.py', option1, option2, _output_filename]\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"languages_amount_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        execfile('count_language.py')\n",
    "        print \"\\n\"\n",
    "\n",
    "    time_end = time.clock()\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "try:\n",
    "    from nltk import wordpunct_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "except ImportError:\n",
    "    print '[!] You need to install nltk (http://nltk.org/index.html)'\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def _calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Dictionary with languages and unique stopwords seen in analyzed text\n",
    "    @rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return the highest scored.\n",
    "    \n",
    "    It uses a stopwords based approach, counting how many unique stopwords\n",
    "    are seen in analyzed text.\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Most scored language guessed\n",
    "    @rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "\n",
    "    return most_rated_language\n",
    "\n",
    "\n",
    "def set_language_for_each_tweet(_file):\n",
    "    _data = open(_file, 'r')\n",
    "    open(_file[:-5] + \"_language.data\", 'w').close()\n",
    "    _output = open(_file[:-5] + \"_language.data\", 'a')\n",
    "    for line in _data.readlines():\n",
    "        record = json.loads(line)\n",
    "        language = detect_language(record['text'].encode('utf-8'))\n",
    "        try:\n",
    "            record.update({'language': language})\n",
    "            _output.write(json.dumps(record, sort_keys=True) + \"\\n\")\n",
    "        except:\n",
    "            # Should not happen.\n",
    "            print \"WTF!\"\n",
    "    _data.close()\n",
    "    _output.close()\n",
    "    return _output.name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, record):\n",
    "        yield [record['language'], 1]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield [key, sum(values)]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.max_reducer)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see which hours in which week had more records made in the different languages by platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Graphic **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
