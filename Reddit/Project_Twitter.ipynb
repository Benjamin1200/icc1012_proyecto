{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddite\n",
    "\n",
    "In this notebook we are going to show all the used for the analysis. Also we are going to show all the graphics asociated with the data obtaines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We downloaded data from reddite with various methods, which did not provide the same amount of data, so we standarized to the minimal amount which it could be useful. Those fields are: \n",
    "* Date when it was created at.\n",
    "* ID of the tweet, which is unique.\n",
    "* The text of the comment.\n",
    "* The user, which contains the user screen name (username) and the user ID.\n",
    "\n",
    "After defining that the period of downloading data was finished, the amount of data gathered was: **500000** unique tweets. Which is a decent amount of data to analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "1) The first step is to filter the data in different files that will be used for all the analysis. The filter will be keywords (in the meantime), the keywords are the platforms name (i.e. Nintendo, Playstation, Xbox), these keywords are searched in the text and the username. If there is no defined preference (not zero preference) in which platform the record was pointing to, it is inserted in both files.\n",
    "\n",
    "The ideal method to filter the data would be create a database of keywords asociated with each platform, so in every record when searching it could be calculated the probability of that text (according to all the words) to which platform it goes. This would require a model with N-Bayes, but like most predictive models it requires training, which we do not have at this moment.\n",
    "\n",
    "**Note**: Also from here on we called each file as follows:\n",
    "* project_tweets01.data -> Nintendo\n",
    "* project_tweets02.data -> Playstation\n",
    "* project_tweets03.data -> Xbox\n",
    "* project_tweets04.data -> Else (Which is everything else that did not fit in the other categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\tINPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "\tdef mapper(self, _, record):\n",
    "\t\tdate = datetime.fromtimestamp(int(record['created_at']))\n",
    "\t\tweek = date.isocalendar()[1]\n",
    "\t\tyield week, int(record['created_at'])\n",
    "\n",
    "\tdef reducer(self, key, values):\n",
    "\t\tfor i in values:\n",
    "\t\t\tyield (key, datetime.fromtimestamp(i).hour), 1\n",
    "\tdef max_reducer(self, stat, values):\n",
    "\t\tyield stat, sum(values)\n",
    "\n",
    "\tdef steps(self):\n",
    "\t\treturn [MRStep(mapper=self.mapper, reducer=self.reducer),\n",
    "\t\t\t\tMRStep(reducer=self.max_reducer)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tMRWordFrequencyCount.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The first analysis to do with this data that is now filtered by platforms, is to count the amount of records per platform. This is done with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\tINPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "\tdef mapper(self, _, record):\n",
    "\t\t\n",
    "\t\tyield record['classification'], 1\n",
    "\n",
    "\tdef max_reducer(self, stat, values):\n",
    "\t\tyield stat, sum(values)\n",
    "\n",
    "\tdef steps(self):\n",
    "\t\treturn [MRStep(mapper=self.mapper, reducer=self.max_reducer)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tMRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "project_tweets01.data\n",
    "\n",
    "1317667 \n",
    "\n",
    "project_tweets02.data\n",
    "\n",
    "2337881 \n",
    "\n",
    "project_tweets03.data\n",
    "\n",
    "2260157 \n",
    "\n",
    "project_tweets04.data\n",
    "\n",
    "735410 \n",
    "\n",
    "project_tweets.data\n",
    "\n",
    "6542718 \n",
    "\n",
    "Time taken to completion of the metric: 37.615491 in processor time\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see that in the \"else\" category we have **11.24%** of all the data, which is not a small amount. But considering that our filter for the platforms is kind of brute force is all right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The next analysis to do is to count the amount of unique users per platform. This is done with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    # Clean File.\n",
    "    open(\"user_amount_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for _file in files:\n",
    "        # parameters for mrjob.\n",
    "        # To run your job in multiple subprocesses with a few Hadoop features simulated, use -r local.\n",
    "        option1 = \"\" #\"\"-r\"\n",
    "        option2 = \"\" #\"\"local\"\n",
    "        sys.argv = ['user_amount.py', option1, option2, _file]\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"user_amount_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        execfile('user_amount.py')\n",
    "        print \"\\n\"\n",
    "\n",
    "    time_end = time.clock()\n",
    "\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "import time\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, record):\n",
    "        yield [record['user']['screen_name'], 1]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield [key, 1]\n",
    "\n",
    "    def mapper2(self, key, values):\n",
    "        yield ['amount_users', values]\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        yield [key, sum(values)]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer),\n",
    "                MRStep(mapper=self.mapper2, reducer=self.reducer2)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #time_start = time.clock()\n",
    "    MRWordFrequencyCount().run()\n",
    "    #time_end = time.clock()\n",
    "    #print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "project_tweets01.data\n",
    "\n",
    "\"amount_users\"\t426450\n",
    "\n",
    "\n",
    "project_tweets02.data\n",
    "\n",
    "\"amount_users\"\t574308\n",
    "\n",
    "\n",
    "project_tweets03.data\n",
    "\n",
    "\"amount_users\"\t727273\n",
    "\n",
    "\n",
    "project_tweets04.data\n",
    "\n",
    "\"amount_users\"\t265827\n",
    "\n",
    "\n",
    "Time taken to completion of the metric: 426.030981 in processor time\n",
    "\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see that in the Xbox platform there are more unique users than in all the other platforms by a not small percentage, it almost duplicates Nintendo unique users. What is interesting, is using the analysis from before we can see that Xbox had less records than Playstation, but seeing this there is a whooping 150k (estimated) more unique users in Xbox, which could lead us that Playstation content creation is more for each unique user or that there is a tiny amount of users that produce all the content for this platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) The next analysis is to calculate the Top 10 users that generate the most content in each platform. This is done with the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.clock()\n",
    "    # Clean File.\n",
    "    open(\"top_users_by_platform_summary.txt\", 'w').close()\n",
    "    files = [\"project_tweets01.data\", \"project_tweets02.data\", \"project_tweets03.data\", \"project_tweets04.data\"]\n",
    "    for _file in files:\n",
    "        # parameters for mrjob.\n",
    "        # To run your job in multiple subprocesses with a few Hadoop features simulated, use -r local.\n",
    "        option1 = \"\" #\"\"-r\"\n",
    "        option2 = \"\" #\"\"local\"\n",
    "        sys.argv = ['top_users.py', option1, option2, _file]\n",
    "        # Write to file in append mode.\n",
    "        _fo = open(\"top_users_by_platform_summary.txt\", 'a')\n",
    "        sys.stdout = _fo\n",
    "        print _file\n",
    "        execfile('top_users.py')\n",
    "        print \"\\n\"\n",
    "\n",
    "    time_end = time.clock()\n",
    "\n",
    "    print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "import time\n",
    "import itertools\n",
    "import operator\n",
    "import sys\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, record):\n",
    "        yield [record['user']['screen_name'], 1]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield [\"top_user\", (sum(values), key)]\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        user_ids = []\n",
    "        user_tweets = []\n",
    "        for value in values:\n",
    "            user_ids.append(value[1])\n",
    "            user_tweets.append(value[0])\n",
    "        user = {}\n",
    "        for i in xrange(0, len(user_ids)):\n",
    "            user[user_ids[i]] = user_tweets[i]\n",
    "        top_users = sorted(user.items(), key=lambda x: (x[1], operator.itemgetter(0)), reverse=True)\n",
    "        for user in top_users[0:10]:\n",
    "            #print user[0], user[1]\n",
    "            yield [user[0], user[1]]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer),\n",
    "                MRStep(reducer=self.reducer2)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #time_start = time.clock()\n",
    "    MRWordFrequencyCount().run()\n",
    "    #time_end = time.clock()\n",
    "    #print \"Time taken to completion of the metric: {0} in processor time\".format(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "project_tweets01.data\n",
    "* \"savetimeandmoey\"\t11391\n",
    "* \"AuctionPorn\"\t8449\n",
    "* \"AmazonBay4u\"\t8352\n",
    "* \"retrodeals\"\t7435\n",
    "* \"Nintendo_Legend\"\t6895\n",
    "* \"retrodealsUK\"\t6744\n",
    "* \"LastChanceGamer\"\t5904\n",
    "* \"GameUP247\"\t5580\n",
    "* \"RetroNuss\"\t5441\n",
    "* \"Nintendoe3E3\"\t5438\n",
    "\n",
    "\n",
    "project_tweets02.data\n",
    "* \"Cammie_Whybrew\"\t13616\n",
    "* \"AskPlayStation\"\t13282\n",
    "* \"eBayShopperNews\"\t11578\n",
    "* \"VideoGamesMall\"\t10274\n",
    "* \"savetimeandmoey\"\t8632\n",
    "* \"collinschristof\"\t5382\n",
    "* \"topnewskoeln\"\t5297\n",
    "* \"Gamifive\"\t4829\n",
    "* \"Xbox_360_Gamez\"\t4080\n",
    "* \"pressebank\"\t3799\n",
    "\n",
    "\n",
    "project_tweets03.data\n",
    "* \"Xbox_360_Gamez\"\t34137\n",
    "* \"XboxSupport\"\t14421\n",
    "* \"VideoGamesMall\"\t11835\n",
    "* \"Xbox_One_Reddit\"\t10215\n",
    "* \"xboxgamersdeals\"\t9224\n",
    "* \"GameUP247\"\t9156\n",
    "* \"KingsleyNewz\"\t9139\n",
    "* \"bullzyy\"\t7576\n",
    "* \"savetimeandmoey\"\t7176\n",
    "* \"giveawayxfab\"\t7028\n",
    "\n",
    "\n",
    "project_tweets04.data\n",
    "* \"savetimeandmoey\"\t15483\n",
    "* \"VideoGames_Up\"\t13804\n",
    "* \"giveawaygigatop\"\t8878\n",
    "* \"tw100_1\"\t6538\n",
    "* \"videogames_pt\"\t6357\n",
    "* \"videogames_fr\"\t5993\n",
    "* \"giveawayxfab\"\t5886\n",
    "* \"DMGG_Videogames\"\t5295\n",
    "* \"ShoppeWorld\"\t4824\n",
    "* \"VideoGames_TV\"\t3709\n",
    "\n",
    "\n",
    "Time taken to completion of the metric: 335.930534 in processor time\n",
    "\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "From this we can see that (requires more analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Another analysis would be ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
